{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.26/06\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "import numpy as np\n",
    "from root_data_loader import load_data\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('source /cms/ldap_home/yeonjoon/working_dir/VcbMVAStudy/setup.sh')\n",
    "filepath = 'root://cluster142.knu.ac.kr//store/user/yeonjoon/Vcb_2018_Mu_Reco_Tree.root'\n",
    "f = ROOT.TFile()\n",
    "f = f.Open(filepath, 'READ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cppyy.gbl.TBranch object at 0x55de4666a7c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = f.Get('Reco_45')\n",
    "MVA_Score = np.array([0], dtype=np.float64)\n",
    "tree.Branch(\"MVA_Score_template\", MVA_Score, 'normal/D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root://cluster142.knu.ac.kr//store/user/yeonjoon/Vcb_2018_Mu_Reco_Tree.root\n",
      "Full dataset, For validation\n",
      "[0.0053459  0.0043507  0.03749209 0.03283206 0.8746199  0.88254017\n",
      " 0.         1.        ]\n",
      "['n_bjets', 'n_cjets']\n"
     ]
    }
   ],
   "source": [
    "varlist = ['bvsc_w_u','bvsc_w_d','cvsl_w_u','cvsl_w_d','cvsb_w_u','cvsb_w_d','n_bjets','n_cjets','weight']\n",
    "data = load_data(file_path=filepath,varlist=varlist,test_ratio=0,val_ratio=0,sigTree=['Reco_45'],bkgTree=['Reco_43'])\n",
    "print(data['cat_columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cms/ldap_home/yeonjoon/miniconda3/envs/ML-torch/lib/python3.10/site-packages/ROOT/_facade.py:153: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  return _orig_ihook(name, *args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cms/ldap_home/yeonjoon/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "print(data['cat_dims'])\n",
    "model = TabNetClassifier()\n",
    "model.load_model('/cms/ldap_home/yeonjoon/working_dir/VcbMVAStudy/TabNet_template/model_largebatch_nosmote.pt.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cms/ldap_home/yeonjoon/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "print(data['cat_dims'])\n",
    "model = TabNetClassifier()\n",
    "model.load_model('/cms/ldap_home/yeonjoon/working_dir/VcbMVAStudy/TabNet_template/model_largebatch_nosmote.pt.zip')\n",
    "def getMVA(bvsc_w_u,bvsc_w_d,cvsl_w_u,cvsl_w_d,cvsb_w_u,cvsb_w_d,n_bjets,n_cjets):\n",
    "\n",
    "    train_features=[bvsc_w_u,\n",
    "                    bvsc_w_d,\n",
    "                    cvsl_w_u,\n",
    "                    cvsl_w_d,\n",
    "                    cvsb_w_u,\n",
    "                    cvsb_w_d,\n",
    "                    data['cat_labelencoder']['n_bjets'].transform([n_bjets])[0],\n",
    "                    data['cat_labelencoder']['n_cjets'].transform([n_cjets])[0]]\n",
    "    train_features = np.array(train_features)\n",
    "    \n",
    "    return model.predict_proba([train_features])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = ROOT.TFile(\"out.root\",\"RECREATE\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list = ['Reco_45','Reco_43','Reco_41','Reco_23','Reco_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23521it [07:09, 54.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i, entry \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm\u001b[39m.\u001b[39mtqdm(tree)):\n\u001b[1;32m     12\u001b[0m     weights[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(entry\u001b[39m.\u001b[39mweight)\n\u001b[0;32m---> 13\u001b[0m     MVAs[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(getMVA(entry\u001b[39m.\u001b[39;49mbvsc_w_u,entry\u001b[39m.\u001b[39;49mbvsc_w_d,entry\u001b[39m.\u001b[39;49mcvsl_w_u,entry\u001b[39m.\u001b[39;49mcvsl_w_d,entry\u001b[39m.\u001b[39;49mcvsb_w_u,entry\u001b[39m.\u001b[39;49mcvsb_w_d,entry\u001b[39m.\u001b[39;49mn_bjets,entry\u001b[39m.\u001b[39;49mn_cjets))\n\u001b[1;32m     14\u001b[0m     tree_out\u001b[39m.\u001b[39mFill()\n\u001b[1;32m     15\u001b[0m f_out\u001b[39m.\u001b[39mcd()\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mgetMVA\u001b[0;34m(bvsc_w_u, bvsc_w_d, cvsl_w_u, cvsl_w_d, cvsb_w_u, cvsb_w_d, n_bjets, n_cjets)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_features\u001b[39m=\u001b[39m[bvsc_w_u,\n\u001b[1;32m      8\u001b[0m                 bvsc_w_d,\n\u001b[1;32m      9\u001b[0m                 cvsl_w_u,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 data[\u001b[39m'\u001b[39m\u001b[39mcat_labelencoder\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mn_bjets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtransform([n_bjets])[\u001b[39m0\u001b[39m],\n\u001b[1;32m     14\u001b[0m                 data[\u001b[39m'\u001b[39m\u001b[39mcat_labelencoder\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mn_cjets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtransform([n_cjets])[\u001b[39m0\u001b[39m]]\n\u001b[1;32m     15\u001b[0m train_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(train_features)\n\u001b[0;32m---> 17\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mpredict_proba([train_features])[\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/tab_model.py:102\u001b[0m, in \u001b[0;36mTabNetClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m batch_nb, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m    100\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m--> 102\u001b[0m     output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(data)\n\u001b[1;32m    103\u001b[0m     predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)(output)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    104\u001b[0m     results\u001b[39m.\u001b[39mappend(predictions)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[0;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:471\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    470\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 471\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    472\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[1;32m    475\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:168\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m# output\u001b[39;00m\n\u001b[1;32m    167\u001b[0m masked_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(M, x)\n\u001b[0;32m--> 168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeat_transformers[step](masked_x)\n\u001b[1;32m    169\u001b[0m d \u001b[39m=\u001b[39m ReLU()(out[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_d])\n\u001b[1;32m    170\u001b[0m steps_output\u001b[39m.\u001b[39mappend(d)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:707\u001b[0m, in \u001b[0;36mFeatTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    706\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared(x)\n\u001b[0;32m--> 707\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspecifics(x)\n\u001b[1;32m    708\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ML-torch/lib/python3.10/site-packages/pytorch_tabnet/tab_network.py:750\u001b[0m, in \u001b[0;36mGLU_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[39mfor\u001b[39;00m glu_id \u001b[39min\u001b[39;00m layers_left:\n\u001b[1;32m    749\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39madd(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglu_layers[glu_id](x))\n\u001b[0;32m--> 750\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m scale\n\u001b[1;32m    751\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from array import array\n",
    "for name in tree_list:\n",
    "    tree = f.Get(name)\n",
    "    tree_out = ROOT.TTree(name,name)\n",
    "    MVAs = array('d', [0])\n",
    "    weights = array('d', [0])\n",
    "    tree_out.Branch(\"MVA\",MVAs,\"MVA/D\")\n",
    "    tree_out.Branch(\"weights\",weights,\"weights/D\")\n",
    "\n",
    "    for i, entry in enumerate(tqdm.tqdm(tree)):\n",
    "        weights[0] = float(entry.weight)\n",
    "        MVAs[0] = float(getMVA(entry.bvsc_w_u,entry.bvsc_w_d,entry.cvsl_w_u,entry.cvsl_w_d,entry.cvsb_w_u,entry.cvsb_w_d,entry.n_bjets,entry.n_cjets))\n",
    "        tree_out.Fill()\n",
    "    f_out.cd()\n",
    "    tree_out.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************\n",
      "*Tree    :Reco_45   : Reco_45                                                *\n",
      "*Entries :   164280 : Total =        15169530 bytes  File  Size =   10952940 *\n",
      "*        :          : Tree compression factor =   1.38                       *\n",
      "******************************************************************************\n",
      "*Br    0 :weight    : weight/F                                               *\n",
      "*Entries :   164280 : Total  Size=     659445 bytes  File Size  =     572561 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.15     *\n",
      "*............................................................................*\n",
      "*Br    1 :n_jets    : n_jets/I                                               *\n",
      "*Entries :   164280 : Total  Size=     659445 bytes  File Size  =     105347 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   6.25     *\n",
      "*............................................................................*\n",
      "*Br    2 :n_bjets   : n_bjets/I                                              *\n",
      "*Entries :   164280 : Total  Size=     659470 bytes  File Size  =      81370 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   8.10     *\n",
      "*............................................................................*\n",
      "*Br    3 :n_cjets   : n_cjets/I                                              *\n",
      "*Entries :   164280 : Total  Size=     659470 bytes  File Size  =     107418 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   6.13     *\n",
      "*............................................................................*\n",
      "*Br    4 :best_mva_score : best_mva_score/F                                  *\n",
      "*Entries :   164280 : Total  Size=     659645 bytes  File Size  =     575377 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.15     *\n",
      "*............................................................................*\n",
      "*Br    5 :pt_had_t_b : pt_had_t_b/F                                          *\n",
      "*Entries :   164280 : Total  Size=     659545 bytes  File Size  =     588204 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.12     *\n",
      "*............................................................................*\n",
      "*Br    6 :pt_w_u    : pt_w_u/F                                               *\n",
      "*Entries :   164280 : Total  Size=     659445 bytes  File Size  =     588182 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.12     *\n",
      "*............................................................................*\n",
      "*Br    7 :pt_w_d    : pt_w_d/F                                               *\n",
      "*Entries :   164280 : Total  Size=     659445 bytes  File Size  =     587986 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.12     *\n",
      "*............................................................................*\n",
      "*Br    8 :pt_lep_t_b : pt_lep_t_b/F                                          *\n",
      "*Entries :   164280 : Total  Size=     659545 bytes  File Size  =     588442 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.12     *\n",
      "*............................................................................*\n",
      "*Br    9 :eta_had_t_b : eta_had_t_b/F                                        *\n",
      "*Entries :   164280 : Total  Size=     659570 bytes  File Size  =     611406 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.08     *\n",
      "*............................................................................*\n",
      "*Br   10 :eta_w_u   : eta_w_u/F                                              *\n",
      "*Entries :   164280 : Total  Size=     659470 bytes  File Size  =     610839 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.08     *\n",
      "*............................................................................*\n",
      "*Br   11 :eta_w_d   : eta_w_d/F                                              *\n",
      "*Entries :   164280 : Total  Size=     659470 bytes  File Size  =     610764 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.08     *\n",
      "*............................................................................*\n",
      "*Br   12 :eta_lep_t_b : eta_lep_t_b/F                                        *\n",
      "*Entries :   164280 : Total  Size=     659570 bytes  File Size  =     610819 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.08     *\n",
      "*............................................................................*\n",
      "*Br   13 :bvsc_had_t_b : bvsc_had_t_b/F                                      *\n",
      "*Entries :   164280 : Total  Size=     659595 bytes  File Size  =     567867 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.16     *\n",
      "*............................................................................*\n",
      "*Br   14 :bvsc_w_u  : bvsc_w_u/F                                             *\n",
      "*Entries :   164280 : Total  Size=     659495 bytes  File Size  =     604555 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.09     *\n",
      "*............................................................................*\n",
      "*Br   15 :cvsb_w_u  : cvsb_w_u/F                                             *\n",
      "*Entries :   164280 : Total  Size=     659495 bytes  File Size  =     567552 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.16     *\n",
      "*............................................................................*\n",
      "*Br   16 :cvsl_w_u  : cvsl_w_u/F                                             *\n",
      "*Entries :   164280 : Total  Size=     659495 bytes  File Size  =     593668 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.11     *\n",
      "*............................................................................*\n",
      "*Br   17 :bvsc_w_d  : bvsc_w_d/F                                             *\n",
      "*Entries :   164280 : Total  Size=     659495 bytes  File Size  =     593484 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.11     *\n",
      "*............................................................................*\n",
      "*Br   18 :cvsb_w_d  : cvsb_w_d/F                                             *\n",
      "*Entries :   164280 : Total  Size=     659495 bytes  File Size  =     609711 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.08     *\n",
      "*............................................................................*\n",
      "*Br   19 :cvsl_w_d  : cvsl_w_d/F                                             *\n",
      "*Entries :   164280 : Total  Size=     659495 bytes  File Size  =     589737 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.12     *\n",
      "*............................................................................*\n",
      "*Br   20 :bvsc_lep_t_b : bvsc_lep_t_b/F                                      *\n",
      "*Entries :   164280 : Total  Size=     659595 bytes  File Size  =     572070 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression=   1.15     *\n",
      "*............................................................................*\n",
      "*Br   21 :m_w_u     : m_w_u/F                                                *\n",
      "*Entries :   164280 : Total  Size=     659420 bytes  File Size  =       5150 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression= 127.90     *\n",
      "*............................................................................*\n",
      "*Br   22 :m_w_d     : m_w_d/F                                                *\n",
      "*Entries :   164280 : Total  Size=     659420 bytes  File Size  =       5150 *\n",
      "*Baskets :       21 : Basket Size=      32000 bytes  Compression= 127.90     *\n",
      "*............................................................................*\n",
      "*Br   23 :MVA_Score_template : normal/D                                      *\n",
      "*Entries :        0 : Total  Size=        509 bytes  One basket in memory    *\n",
      "*Baskets :        0 : Basket Size=      32000 bytes  Compression=   1.00     *\n",
      "*............................................................................*\n"
     ]
    }
   ],
   "source": [
    "for name in tree_list:\n",
    "    data = load_data(file_path=filepath,varlist=varlist,test_ratio=0,val_ratio=0,sigTree=['Reco_45'],bkgTree=[])\n",
    "    tree = f.Get(name)\n",
    "    print(tree.GetEntries())\n",
    "    print(data['train_features'].shape)\n",
    "    tree_out = ROOT.TTree(name,name)\n",
    "    MVAs = array('d', [0])\n",
    "    weights = array('d', [0])\n",
    "    tree_out.Branch(\"MVA\",MVAs,\"MVA/D\")\n",
    "    tree_out.Branch(\"weights\",weights,\"weights/D\")\n",
    "    result = model.predict_proba(data['train_features'])[:,1]\n",
    "    \n",
    "    for i, entry in enumerate(result):\n",
    "        weights[0] = float(result[i])\n",
    "        MVAs[0] = float(data['train_weight'][i])\n",
    "        tree_out.Fill()\n",
    "    f_out.cd()\n",
    "    tree_out.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
